{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Didn't work \n",
    "- need to do some hyperparameter tuning for epoch \n",
    "- something is off with the tokenizer code. it isn't flowing as it should "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\chand\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import torch\n",
    "import os\n",
    "from PyPDF2 import PdfReader\n",
    "import re\n",
    "import email \n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to scrape blog content from a website\n",
    "def scrape_blogs(url):\n",
    "    base_url = \"https://www.greatplacetowork.com\"\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    blogs = []\n",
    "    for blog in (soup.find_all('a', class_='link', href=True)):\n",
    "        blog_url = blog['href']\n",
    "        if blog_url.startswith('/resources/blog'):\n",
    "            blog_url = f\"{base_url}{blog_url}\"\n",
    "            #print(\"Found blog link:\", blog_url)  # Print the found blog link\n",
    "            \n",
    "            # Scrape content from blog page\n",
    "            blog_title, blog_content = scrape_blog_content(blog_url)\n",
    "            if blog_content:\n",
    "                #print(\"Found blog content for:\", blog_title)  # Print the found blog title\n",
    "                blogs.append(blog_content.strip())\n",
    "            #else:\n",
    "                #print(\"Blog content not found.\")\n",
    "        #else:\n",
    "            #print(\"Not a blog link.\")\n",
    "\n",
    "    return blogs\n",
    "\n",
    "# Function to scrape content from individual blog pages\n",
    "def scrape_blog_content(blog_url):\n",
    "    response = requests.get(blog_url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Extract blog title from the <title> tag\n",
    "    title_tag = soup.find('title')\n",
    "    if title_tag:\n",
    "        blog_title = title_tag.text.strip()\n",
    "        #print(\"Found blog title:\", blog_title)\n",
    "    else:\n",
    "        #print(\"Blog title not found.\")\n",
    "        blog_title = \"\"\n",
    "    \n",
    "    # Find and extract text content from <p>, <h3>, <strong>, etc.\n",
    "    content_elements = soup.find_all(['p', 'h3', 'strong'])\n",
    "    content_text = ' '.join([element.get_text() for element in content_elements])\n",
    "    \n",
    "    return blog_title, content_text\n",
    "\n",
    "# Function to scrape content from a set of links\n",
    "def scrape_links(links):\n",
    "    contents = []\n",
    "    for link in tqdm(links,desc=\"Scrapping Links\"):\n",
    "        response = requests.get(link)\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            # Customize this based on the structure of the content you want to scrape from the links\n",
    "            # For example:\n",
    "            # content = soup.find('div', class_='content').get_text()\n",
    "            content = soup.get_text()\n",
    "            contents.append(content.strip())\n",
    "        else:\n",
    "            print(\"Failed to retrieve content from link:\", link)\n",
    "    return contents\n",
    "\n",
    "# Function to scrape text content from PDF files in a folder\n",
    "def scrape_pdf_content(folder_path):\n",
    "    pdf_contents = []\n",
    "    for filename in tqdm(os.listdir(folder_path),desc=\"Scrapping PDFs\"):\n",
    "        if filename.endswith(\".pdf\"):\n",
    "            pdf_path = os.path.join(folder_path, filename)\n",
    "            print(\"Scraping content from PDF:\", pdf_path)\n",
    "            \n",
    "            with open(pdf_path, \"rb\") as pdf_file:\n",
    "                pdf_reader = PdfReader(pdf_file)\n",
    "                text = \"\"\n",
    "                for page_num in range(len(pdf_reader.pages)):\n",
    "                    text += pdf_reader.pages[page_num].extract_text()\n",
    "                \n",
    "                pdf_contents.append(text.strip())\n",
    "    \n",
    "    return pdf_contents\n",
    "\n",
    "# Function to scrape text content from emails in a folder\n",
    "def scrape_email_content(folder_path):\n",
    "    email_contents = []\n",
    "    for filename in tqdm(os.listdir(folder_path),desc=\"Scrapping Emails\"):\n",
    "        if filename.endswith(\".eml\"):  # Change the file extension to .eml\n",
    "            email_path = os.path.join(folder_path, filename)\n",
    "            print(\"Scraping content from email:\", email_path)\n",
    "            \n",
    "            with open(email_path, \"r\") as email_file:\n",
    "                msg = email.message_from_file(email_file)\n",
    "                # Extract text content from the email\n",
    "                email_content = \"\"\n",
    "                for part in msg.walk():\n",
    "                    if part.get_content_type() == \"text/plain\":\n",
    "                        email_content += part.get_payload()\n",
    "                \n",
    "                email_contents.append(email_content.strip())\n",
    "    \n",
    "    return email_contents\n",
    "\n",
    "# Function to fine-tune GPT-2 model on scraped blog content\n",
    "def fine_tune_model(scraped_content):\n",
    "    print(\"Fine tuning model\")\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "    model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "    # Tokenize content\n",
    "    tokenized_content = [tokenizer.encode(content, return_tensors=\"pt\", max_length=512, truncation=True) for content in scraped_content]\n",
    "\n",
    "    # Fine-tune model\n",
    "    model.train()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "    for epoch in tqdm(range(100),desc=\"Model optimization\"):  # adjust number of epochs as needed\n",
    "        for batch in tokenized_content:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(input_ids=batch, labels=batch)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Function to generate response given input text\n",
    "def generate_response(input_text, model, tokenizer):\n",
    "    # Tokenize input text\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "    attention_mask = torch.ones_like(input_ids)\n",
    "\n",
    "    # Generate response\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(input_ids, attention_mask=attention_mask, max_length=100, pad_token_id=tokenizer.eos_token_id, num_return_sequences=1)\n",
    "\n",
    "    # Decode response\n",
    "    response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scrapping blogs: 100%|██████████| 90/90 [09:44<00:00,  6.49s/it]\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Scrapping Links: 100%|██████████| 3/3 [00:45<00:00, 15.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping content from PDF: C:/Users/chand/OneDrive/Documents 1/chatgptw/pdfs_training\\how-to-create-an-innovation-by-all-culture-innovation-insights-4.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping content from PDF: C:/Users/chand/OneDrive/Documents 1/chatgptw/pdfs_training\\innovation-by-all-innovation-insights-1.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping content from PDF: C:/Users/chand/OneDrive/Documents 1/chatgptw/pdfs_training\\innovation-everywhere-innovation-insights-3.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping content from PDF: C:/Users/chand/OneDrive/Documents 1/chatgptw/pdfs_training\\the-five-hidden-barriers-to-innovation-innovation-insights-2.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping content from PDF: C:/Users/chand/OneDrive/Documents 1/chatgptw/pdfs_training\\women-in-the-workplace-2019-best-workplaces-for-women-report.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scrapping PDFs: 100%|██████████| 5/5 [00:03<00:00,  1.52it/s]\n",
      "Scrapping Emails: 100%|██████████| 2/2 [00:00<00:00, 49.29it/s]\n",
      "Scraping Progress:   0%|          | 0/90 [10:33<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping content from email: C:/Users/chand/OneDrive/Documents 1/chatgptw/emails_scrapping\\Qs for Deloitte Fireside Chat (1).eml\n",
      "Scraping content from email: C:/Users/chand/OneDrive/Documents 1/chatgptw/emails_scrapping\\Seeking Guidance.eml\n",
      "Fine tuning model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model optimization: 100%|██████████| 100/100 [43:35:15<00:00, 1569.16s/it]  \n"
     ]
    }
   ],
   "source": [
    "# Main function\n",
    "if __name__ == \"__main__\":\n",
    "    url = \"https://www.greatplacetowork.com/resources/all-insights\"\n",
    "    num_pages = 90  # Adjust this based on the total number of pages\n",
    "\n",
    "    # Initialize the overall progress bar\n",
    "    overall_progress_bar = tqdm(total=num_pages, desc=\"Scraping Progress\")\n",
    "    \n",
    "    # Scraping blogs from multiple pages\n",
    "    all_blogs = []\n",
    "    for i in tqdm(range(num_pages),desc=\"Scrapping blogs\"):\n",
    "        page_url = f\"{url}?start={i * 12}\"\n",
    "        #print(\"Scraping page:\", page_url)\n",
    "        page_blogs = scrape_blogs(page_url)\n",
    "        all_blogs.extend(page_blogs)\n",
    "\n",
    "    # Scrape content from a set of links\n",
    "    links = [\n",
    "        \"https://cloud.kapostcontent.net/pub/2f4c3763-0315-4f06-a947-0213596c2f04/2020-worlds-best-workplaces-rising-to-historic-challenges?kui=WXQe4R4ni4cOBETV8ABaEg\",\n",
    "        \"https://cloud.kapostcontent.net/pub/3c6877ba-67b9-400d-ad8c-439b609d9999/the-future-of-work-is-for-all-2020-fortune-100-best-companies-report?kui=8V47uCOLuNVcSP_ZMaYlpQ\",\n",
    "        \"https://cloud.kapostcontent.net/pub/553e8f75-a21d-4551-b42a-676840a73323/d-and-i-puzzle?kui=-U-DLNYkmOsDDvb7rhMjFQ\"\n",
    "    ]\n",
    "    scraped_links = scrape_links(links)\n",
    "\n",
    "    # Scrape text content from PDF files in a folder\n",
    "    pdf_folder_path = \"C:/Users/chand/OneDrive/Documents 1/chatgptw/pdfs_training\"\n",
    "    scraped_pdfs = scrape_pdf_content(pdf_folder_path)\n",
    "\n",
    "    # Scrape text content from emails in a folder\n",
    "    email_folder_path = \"C:/Users/chand/OneDrive/Documents 1/chatgptw/emails_scrapping\"\n",
    "    scraped_emails = scrape_email_content(email_folder_path)\n",
    "\n",
    "    # Close the overall progress bar\n",
    "    overall_progress_bar.close()\n",
    "\n",
    "    # Combine all scraped content\n",
    "    all_scraped_content = all_blogs + scraped_links + scraped_pdfs + scraped_emails\n",
    "\n",
    "    # Fine-tune GPT-2 model on scraped content\n",
    "    model = fine_tune_model(all_scraped_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[0;32m      2\u001b[0m input_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhat\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms the definition of innovation?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 3\u001b[0m response \u001b[38;5;241m=\u001b[39m generate_response(input_text, model,\u001b[43mtokenizer\u001b[49m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerated response:\u001b[39m\u001b[38;5;124m\"\u001b[39m, response)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "input_text = \"what's the definition of innovation?\"\n",
    "response = generate_response(input_text, model,tokenizer)\n",
    "print(\"Generated response:\", response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
